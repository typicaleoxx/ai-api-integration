{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b2dc25f",
   "metadata": {},
   "source": [
    "## AI API Integration - Text Generation Demo\n",
    "\n",
    "This notebook demonstrates how to connect to an AI API, send a prompt, and receive a generated text response.\n",
    "\n",
    "- Install required libraries\n",
    "- Load the API key securely from a `.env` file\n",
    "- Initialize the AI client\n",
    "- Send a prompt\n",
    "- Print the generated response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e524de",
   "metadata": {},
   "source": [
    "### Install Required Dependencies\n",
    "\n",
    "This command installs the external libraries needed for this project.\n",
    "\n",
    "- `google-genai` -> The official client library used to communicate with the AI model.\n",
    "- `python-dotenv` -> Allows us to securely load environment variables (such as API keys) from a `.env` file.\n",
    "\n",
    "The `%pip` command is a Jupyter-specific magic command that installs packages directly into the notebook's active Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9314e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-genai in /home/sneha/anaconda3/lib/python3.13/site-packages (1.64.0)\n",
      "Requirement already satisfied: python-dotenv in /home/sneha/anaconda3/lib/python3.13/site-packages (1.1.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (4.10.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai) (2.48.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (2.12.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (4.15.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (1.3.0)\n",
      "Requirement already satisfied: aiohttp>=3.10.11 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (3.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/sneha/anaconda3/lib/python3.13/site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: cryptography>=38.0.3 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (46.0.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in /home/sneha/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/sneha/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/sneha/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/sneha/anaconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/sneha/anaconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sneha/anaconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sneha/anaconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/sneha/anaconda3/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (0.6.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from aiohttp>=3.10.11->google-genai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from aiohttp>=3.10.11->google-genai) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from aiohttp>=3.10.11->google-genai) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/sneha/anaconda3/lib/python3.13/site-packages (from aiohttp>=3.10.11->google-genai) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sneha/anaconda3/lib/python3.13/site-packages (from aiohttp>=3.10.11->google-genai) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from aiohttp>=3.10.11->google-genai) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from aiohttp>=3.10.11->google-genai) (1.22.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /home/sneha/anaconda3/lib/python3.13/site-packages (from cffi>=2.0.0->cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (2.23)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-genai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092ca789",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "\n",
    "We import the necessary modules for this project:\n",
    "\n",
    "- `google.genai` -> Used to create the AI client and send requests to the model.\n",
    "- `os` -> Provides access to environment variables stored in the system.\n",
    "- `load_dotenv` -> Loads variables from a `.env` file into the environment securely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6dc6431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1350a57",
   "metadata": {},
   "source": [
    "### Load Environment Variables\n",
    "\n",
    "Instead of hardcoding the API key directly into the script (which is insecure),\n",
    "we store it in a `.env` file.\n",
    "\n",
    "This keeps sensitive information safe and prevents accidental exposure in public repositories.\n",
    "\n",
    "The `.env` file should contain:\n",
    "\n",
    "GEMINI_API_KEY=actual_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "553b30cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path = '.env')\n",
    "API_KEY = os.environ[\"GEMINI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71eb097",
   "metadata": {},
   "source": [
    "### Initialize the AI Client\n",
    "\n",
    "We create a client object using the API key.\n",
    "\n",
    "This client acts as the connection between our application and the AI service.\n",
    "All requests to generate content will be made through this client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0d58381",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2877d456",
   "metadata": {},
   "source": [
    "### Send a Prompt to the Model\n",
    "\n",
    "Here, we send a text prompt to the AI model.\n",
    "\n",
    "- `model` specifies which AI model to use.\n",
    "- `contents` is the input text (prompt) we want the model to respond to.\n",
    "\n",
    "The model processes the prompt and generates a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "785f92a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = \"Hii I am trying to learn RAG from scratch. Please give me simplest explaination about RAG and all context around RAG where how why when is it used.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b7ad39",
   "metadata": {},
   "source": [
    "### Display the Generated Response\n",
    "\n",
    "The result returned from the model is stored in the `response` object.\n",
    "\n",
    "We access the generated text using:\n",
    "\n",
    "response.text\n",
    "\n",
    "This prints the AI-generated output to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "695f5cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hii! I'm excited to help you learn RAG from scratch. Let's break it down into the simplest terms possible.\n",
      "\n",
      "---\n",
      "\n",
      "## The Simplest Explanation: What is RAG?\n",
      "\n",
      "Imagine you have a super smart friend who knows a lot about general topics (that's your **Large Language Model - LLM** like ChatGPT). This friend is great at answering common questions, but sometimes:\n",
      "1.  They might **make things up** (hallucinate).\n",
      "2.  They only know things up to when they *learned* (their training data is fixed, so they don't know recent events).\n",
      "3.  They don't know *your specific, private information* (like your company's internal documents or your personal notes).\n",
      "\n",
      "Now, imagine you give your smart friend a **library full of specific books and documents** (that's your **custom knowledge base**). When you ask your friend a question, you tell them:\n",
      "\n",
      "\"Hey, before you answer, go check these specific books in the library for relevant information. Read those parts, and then give me an answer based *both* on what you already know *and* what you found in the books.\"\n",
      "\n",
      "**That's exactly what RAG does!**\n",
      "\n",
      "*   **R**etrieval: Go to the library (your knowledge base) and find relevant information.\n",
      "*   **A**ugmented: Add that found information to the question you're asking the smart friend (the LLM).\n",
      "*   **G**eneration: Let the smart friend (LLM) generate an answer using *both* their existing knowledge *and* the specific information you just gave them.\n",
      "\n",
      "The goal? To make the smart friend (LLM) **more accurate, more up-to-date, and more specific** to your particular needs, without having to \"retrain\" them entirely.\n",
      "\n",
      "---\n",
      "\n",
      "## Why is RAG Needed? (The \"Why\")\n",
      "\n",
      "RAG solves several critical problems with using LLMs directly:\n",
      "\n",
      "1.  **Hallucinations:** LLMs sometimes confidently generate incorrect or nonsensical information. RAG grounds their answers in factual, retrieved data, significantly reducing this.\n",
      "2.  **Outdated Information:** LLMs are trained on data up to a certain point. RAG allows them to access the latest news, research, or internal documents that were created *after* their training cut-off.\n",
      "3.  **Lack of Specificity/Domain Knowledge:** An LLM knows general medicine, but not your specific patient's medical history or your company's proprietary product details. RAG gives it access to this private, specialized data.\n",
      "4.  **Transparency & Trust:** With RAG, you can often show the source documents the LLM used to formulate its answer, increasing trust and allowing users to verify information.\n",
      "5.  **Cost & Complexity:** Retraining or fine-tuning a large LLM on new data is very expensive and time-consuming. RAG provides a much more agile and cheaper way to update an LLM's knowledge.\n",
      "6.  **Data Privacy:** Your private documents don't need to be sent to the LLM vendor for training. They stay within your control, and only relevant snippets are passed to the LLM for a specific query.\n",
      "\n",
      "---\n",
      "\n",
      "## How Does RAG Work? (The \"How\" - Step-by-Step)\n",
      "\n",
      "There are two main phases in a RAG system:\n",
      "\n",
      "### Phase 1: Building Your \"Library\" (Preparation)\n",
      "\n",
      "This happens *before* anyone asks a question.\n",
      "\n",
      "1.  **Collect Your Documents:** Gather all the data you want the LLM to know about (PDFs, Word documents, website articles, database entries, internal memos, etc.).\n",
      "2.  **Split into Chunks:** Break these large documents into smaller, manageable pieces (e.g., paragraphs, sections, or a few sentences). *Why?* It's easier to find specific relevant bits than an entire book.\n",
      "3.  **Create Embeddings (Indexing):** For each small chunk, convert its meaning into a numerical representation called a \"vector embedding.\" Think of this like creating a super-smart index card for each chunk that captures its topic and context. Chunks with similar meanings will have similar numerical representations.\n",
      "4.  **Store in a Vector Database:** Store these numerical embeddings (along with the original text chunks) in a special database called a \"vector database.\" This database is highly optimized for quickly finding similar embeddings.\n",
      "\n",
      "### Phase 2: Answering a Question (During Interaction)\n",
      "\n",
      "This happens every time a user asks a question.\n",
      "\n",
      "1.  **User Asks a Question:** \"What are the Q3 sales figures for Product X?\"\n",
      "2.  **Embed the Question:** The user's question is also converted into a numerical vector embedding, just like the document chunks.\n",
      "3.  **Retrieve Relevant Chunks:** The RAG system searches the vector database to find the document chunks whose embeddings are most similar to the question's embedding. These are the \"most relevant books\" from your library.\n",
      "4.  **Augment the Prompt:** The retrieved text chunks are then combined with the original user's question to create a new, enhanced prompt for the LLM. It looks something like this:\n",
      "    ```\n",
      "    \"Here is some context about Q3 sales:\n",
      "    [Snippet 1: \"Product X saw a 15% increase in Q3 sales, reaching $1.2M...\"]\n",
      "    [Snippet 2: \"Regional breakdown showed strongest growth in Europe...\"]\n",
      "    [Snippet 3: \"Challenges included supply chain disruptions affecting Product Y...\"]\n",
      "\n",
      "    Based ONLY on the provided context, please answer the following question:\n",
      "    What were the Q3 sales figures for Product X?\"\n",
      "    ```\n",
      "5.  **Generate the Answer:** This augmented prompt is sent to the LLM. The LLM then generates an answer, heavily relying on the specific context you just provided, ensuring its response is accurate and relevant to *your* data.\n",
      "\n",
      "---\n",
      "\n",
      "## Where and When is RAG Used? (The \"Where\" & \"When\")\n",
      "\n",
      "RAG is becoming a foundational technique for making LLMs practically useful in many real-world applications.\n",
      "\n",
      "**Common Use Cases:**\n",
      "\n",
      "*   **Customer Support Chatbots:** Answering customer questions about specific product features, troubleshooting steps, warranty information, or company policies using your knowledge base.\n",
      "*   **Internal Knowledge Bases:** Employees can query internal documents (HR policies, IT guides, project specifications, legal documents) to quickly find answers without sifting through files.\n",
      "*   **Legal Research:** A lawyer can query case law, statutes, and legal precedents to quickly find relevant information for a specific legal argument.\n",
      "*   **Medical Information Systems:** Doctors or researchers can query patient records, medical journals, or drug databases to get precise information (though always with human oversight).\n",
      "*   **Financial Advisory:** Providing tailored advice based on a client's specific portfolio, market reports, and economic forecasts.\n",
      "*   **Personalized Content Recommendation:** For e-commerce, recommending products based on a user's past purchases and specific product descriptions.\n",
      "*   **News and Media Analysis:** Summarizing recent articles on specific topics or answering questions about breaking news using a real-time stream of information.\n",
      "*   **Scientific Research:** Helping scientists explore vast amounts of research papers and data, connecting concepts and summarizing findings.\n",
      "*   **Education:** Students or teachers querying specific textbooks, lecture notes, or educational resources.\n",
      "\n",
      "**Basically, RAG is used anytime you need an LLM to:**\n",
      "\n",
      "*   Provide **highly specific and accurate answers.**\n",
      "*   Access **private or proprietary information.**\n",
      "*   Stay **up-to-date with new information** without costly retraining.\n",
      "*   Offer **explainable and verifiable responses** with sources.\n",
      "\n",
      "---\n",
      "\n",
      "## Key Takeaways\n",
      "\n",
      "*   **RAG bridges the gap** between general LLM knowledge and your specific, real-time, or proprietary data.\n",
      "*   It's a **powerful technique**, not a new type of LLM. It makes existing LLMs much more useful.\n",
      "*   It dramatically **improves accuracy, relevance, and truthfulness** of LLM responses.\n",
      "*   It's an essential component for building **reliable and production-ready AI applications** today.\n",
      "\n",
      "You've taken the first big step in understanding RAG! It's a game-changer for how we interact with AI. Let me know if you have more questions!\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06375d42",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "- Secure API key management using environment variables\n",
    "- Basic AI API integration\n",
    "- Prompt based text generation\n",
    "- Clean separation of explanation and implementation\n",
    "\n",
    "This serves as a foundational example that can be expanded into chatbots, streaming systems, or larger AI-powered applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
