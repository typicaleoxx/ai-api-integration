{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b2dc25f",
   "metadata": {},
   "source": [
    "###  AI API Integration - Text Generation Demo\n",
    "\n",
    "This notebook demonstrates how to connect to an AI API, send a prompt, and receive a generated text response.\n",
    "\n",
    "- Install required libraries\n",
    "- Load the API key securely from a `.env` file\n",
    "- Initialize the AI client\n",
    "- Send a prompt\n",
    "- Print the generated response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e524de",
   "metadata": {},
   "source": [
    "## Install Required Dependencies\n",
    "\n",
    "This command installs the external libraries needed for this project.\n",
    "\n",
    "- `google-genai` -> The official client library used to communicate with the AI model.\n",
    "- `python-dotenv` -> Allows us to securely load environment variables (such as API keys) from a `.env` file.\n",
    "\n",
    "The `%pip` command is a Jupyter-specific magic command that installs packages directly into the notebook's active Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9314e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-genai in /home/sneha/anaconda3/lib/python3.13/site-packages (1.64.0)\n",
      "Requirement already satisfied: python-dotenv in /home/sneha/anaconda3/lib/python3.13/site-packages (1.1.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (4.10.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai) (2.48.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (2.12.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (4.15.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (1.3.0)\n",
      "Requirement already satisfied: aiohttp>=3.10.11 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-genai) (3.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/sneha/anaconda3/lib/python3.13/site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: cryptography>=38.0.3 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (46.0.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/sneha/anaconda3/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in /home/sneha/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/sneha/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/sneha/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/sneha/anaconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/sneha/anaconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sneha/anaconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sneha/anaconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/sneha/anaconda3/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (0.6.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from aiohttp>=3.10.11->google-genai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from aiohttp>=3.10.11->google-genai) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from aiohttp>=3.10.11->google-genai) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/sneha/anaconda3/lib/python3.13/site-packages (from aiohttp>=3.10.11->google-genai) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sneha/anaconda3/lib/python3.13/site-packages (from aiohttp>=3.10.11->google-genai) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from aiohttp>=3.10.11->google-genai) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from aiohttp>=3.10.11->google-genai) (1.22.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /home/sneha/anaconda3/lib/python3.13/site-packages (from cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /home/sneha/anaconda3/lib/python3.13/site-packages (from cffi>=2.0.0->cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (2.23)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-genai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092ca789",
   "metadata": {},
   "source": [
    " Import Required Libraries\n",
    "\n",
    "We import the necessary modules for this project:\n",
    "\n",
    "- `google.genai` -> Used to create the AI client and send requests to the model.\n",
    "- `os` -> Provides access to environment variables stored in the system.\n",
    "- `load_dotenv` -> Loads variables from a `.env` file into the environment securely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc6431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1350a57",
   "metadata": {},
   "source": [
    " Load Environment Variables\n",
    "\n",
    "Instead of hardcoding the API key directly into the script (which is insecure),\n",
    "we store it in a `.env` file.\n",
    "\n",
    "This keeps sensitive information safe and prevents accidental exposure in public repositories.\n",
    "\n",
    "The `.env` file should contain:\n",
    "\n",
    "GEMINI_API_KEY=actual_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "553b30cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path = '.env')\n",
    "API_KEY = os.environ[\"GEMINI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71eb097",
   "metadata": {},
   "source": [
    " Initialize the AI Client\n",
    "\n",
    "We create a client object using the API key.\n",
    "\n",
    "This client acts as the connection between our application and the AI service.\n",
    "All requests to generate content will be made through this client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0d58381",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2877d456",
   "metadata": {},
   "source": [
    "Send a Prompt to the Model\n",
    "\n",
    "Here, we send a text prompt to the AI model.\n",
    "\n",
    "- `model` specifies which AI model to use.\n",
    "- `contents` is the input text (prompt) we want the model to respond to.\n",
    "\n",
    "The model processes the prompt and generates a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f92a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = \"Hii I am trying to learn RAG  and mcp model context protocol from scratch. Please give me simplest explaination about RAG and all context around RAG where how why when is it used\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b7ad39",
   "metadata": {},
   "source": [
    " Display the Generated Response\n",
    "\n",
    "The result returned from the model is stored in the `response` object.\n",
    "\n",
    "We access the generated text using:\n",
    "\n",
    "response.text\n",
    "\n",
    "This prints the AI-generated output to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "695f5cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hii! That's a fantastic goal. RAG is one of the most powerful and practical advancements in working with Large Language Models (LLMs) today. Let's break it down in the simplest terms possible, along with the \"MCP\" aspect.\n",
      "\n",
      "---\n",
      "\n",
      "## Part 1: RAG - Retrieval Augmented Generation\n",
      "\n",
      "### The Simplest Explanation of RAG\n",
      "\n",
      "Imagine you're trying to answer a really specific question, but you only have your memory to rely on. Sometimes your memory is fuzzy, outdated, or just doesn't have the exact fact.\n",
      "\n",
      "Now, imagine you have a superpower: *before* you answer, you can instantly look up relevant information in a library or on the internet, read it, and then use that fresh, accurate information to form your answer.\n",
      "\n",
      "**That superpower is RAG.**\n",
      "\n",
      "*   **R**etrieval: \"Looking up information.\"\n",
      "*   **A**ugmented: \"Adding to\" or \"Enhancing\" your memory/knowledge.\n",
      "*   **G**eneration: \"Forming your answer\" based on the new information.\n",
      "\n",
      "So, RAG is a technique that gives Large Language Models (LLMs) the ability to look up external, up-to-date, and specific information *before* generating a response. It prevents them from \"hallucinating\" (making things up) or giving outdated information.\n",
      "\n",
      "### Why do we need RAG? (The Problems it Solves)\n",
      "\n",
      "LLMs like ChatGPT are incredibly powerful, but they have some inherent limitations:\n",
      "\n",
      "1.  **Outdated Knowledge:** Their knowledge is limited to the data they were trained on, which has a cutoff date (e.g., September 2021 for some versions of GPT-3.5). They don't know about recent events, new discoveries, or current prices.\n",
      "2.  **Hallucination:** LLMs can confidently make up facts that sound plausible but are entirely false. They don't know what they \"don't know,\" so they improvise.\n",
      "3.  **Lack of Specific Domain Knowledge:** They are generalists. They don't have deep, specialized knowledge about your company's internal policies, a specific scientific paper, or a niche legal document.\n",
      "4.  **No Source Attribution:** You can't ask an LLM, \"Where did you get that information?\" Without external data, there's no way to verify its claims.\n",
      "5.  **Cost and Time of Retraining:** Constantly retraining an LLM with new data is extremely expensive and time-consuming. RAG offers a way to update their knowledge base without touching the core model.\n",
      "\n",
      "**RAG addresses all these problems by giving the LLM an external, searchable \"brain\" of up-to-date and specific facts.**\n",
      "\n",
      "### How does RAG work? (The Steps)\n",
      "\n",
      "Think of it as a workflow with a few key stages:\n",
      "\n",
      "1.  **Data Preparation (The \"Library\"):**\n",
      "    *   You take all your external data (documents, PDFs, web pages, databases, etc.) that you want the LLM to access.\n",
      "    *   **Chunking:** You break this data down into smaller, manageable pieces or \"chunks\" (e.g., paragraphs, sections).\n",
      "    *   **Embedding:** Each chunk is converted into a numerical representation called a \"vector embedding.\" These embeddings capture the semantic meaning of the text. Text with similar meanings will have similar vectors.\n",
      "    *   **Vector Database:** These vector embeddings are stored in a special database called a \"vector database.\" This database is highly optimized for searching based on semantic similarity.\n",
      "\n",
      "2.  **Retrieval (Looking it up):**\n",
      "    *   When a user asks a question, that question is *also* converted into a vector embedding.\n",
      "    *   The system then searches the vector database to find the chunks of your external data whose embeddings are most similar to the user's question embedding.\n",
      "    *   These similar chunks are the \"most relevant information\" to the user's query.\n",
      "\n",
      "3.  **Augmentation (Adding to the Question):**\n",
      "    *   The original user's question is *not* sent directly to the LLM.\n",
      "    *   Instead, a *new, augmented prompt* is created. This new prompt combines the user's original question with the relevant chunks of information retrieved from your vector database.\n",
      "    *   Example: \"Answer the following question based *only* on the provided context: [Retrieved Context 1] [Retrieved Context 2] ... User Question: [Original User Question]\"\n",
      "\n",
      "4.  **Generation (Forming the Answer):**\n",
      "    *   This augmented prompt (original question + retrieved context) is sent to the LLM.\n",
      "    *   The LLM now has all the specific, relevant, and up-to-date information it needs *right there in its prompt* to formulate an accurate answer.\n",
      "    *   It's instructed to base its answer *only* on the provided context, minimizing hallucinations.\n",
      "\n",
      "5.  **Response:**\n",
      "    *   The LLM generates the answer, which is then delivered to the user. Often, you can also show the user which sources the information came from (e.g., \"Answer based on Document A, Page 3\").\n",
      "\n",
      "### When is RAG Used? (Use Cases)\n",
      "\n",
      "RAG is incredibly versatile and is used whenever an LLM needs access to dynamic, private, or up-to-date information:\n",
      "\n",
      "*   **Customer Support Chatbots:** Answering questions about product specifics, warranty details, return policies based on a company's internal knowledge base.\n",
      "*   **Internal Knowledge Base Q&A:** Employees asking questions about company HR policies, IT troubleshooting, project documentation, or research papers.\n",
      "*   **Legal & Medical Information:** Providing summaries or answers based on specific legal statutes, case precedents, or medical research articles.\n",
      "*   **Financial Analysis:** Answering questions about specific company reports, market trends, or economic forecasts.\n",
      "*   **Personalized Content Generation:** Generating summaries or recommendations based on a user's specific preferences, past interactions, or private data (securely handled, of course).\n",
      "*   **Research Assistants:** Helping researchers quickly synthesize information from a vast collection of scientific papers or journals.\n",
      "*   **News and Current Events:** Providing up-to-date summaries and answers based on real-time news feeds.\n",
      "\n",
      "### Advantages of RAG\n",
      "\n",
      "*   **Accuracy & Reliability:** Reduces hallucinations and provides factually correct answers.\n",
      "*   **Currency:** Keeps LLMs up-to-date with the latest information without retraining.\n",
      "*   **Domain Specificity:** Allows LLMs to answer questions about proprietary or niche data.\n",
      "*   **Transparency:** Can provide sources for the generated information, increasing trust.\n",
      "*   **Cost-Effective:** Much cheaper than continuously retraining large LLMs.\n",
      "*   **Faster Development:** Can deploy powerful domain-specific LLM applications quickly.\n",
      "\n",
      "### Limitations & Challenges of RAG\n",
      "\n",
      "*   **Quality of Retrieval:** If the retriever fetches irrelevant or poor-quality chunks, the LLM's answer will suffer (\"garbage in, garbage out\").\n",
      "*   **Chunking Strategy:** How you break down your documents (chunk size, overlap) significantly impacts retrieval quality.\n",
      "*   **Latency:** The retrieval step adds a bit of time to the overall response generation.\n",
      "*   **Context Window Limits:** Even with RAG, LLMs have a limit on how much text they can process in a single prompt. If the relevant context is too large, it might exceed this limit.\n",
      "*   **Complexity:** Building a robust RAG system involves managing vector databases, embedding models, and orchestrating the workflow.\n",
      "\n",
      "---\n",
      "\n",
      "## Part 2: MCP - Model Context Protocol (Conceptual)\n",
      "\n",
      "The \"MCP\" (Model Context Protocol) you mentioned isn't a single, universally adopted, strictly defined standard like HTTP or TCP/IP *yet*. It's more of an **emerging conceptual framework or pattern** for how applications interact with and manage the context provided to LLMs, especially as LLMs become more sophisticated and can handle complex inputs and outputs, including tool use.\n",
      "\n",
      "Think of it as the \"rules of engagement\" or the \"structured format\" for how information is packaged and exchanged between your application and the LLM.\n",
      "\n",
      "### What is MCP (Conceptual)?\n",
      "\n",
      "At its core, MCP is about **standardizing and structuring the input (context) and output of an LLM to enable more reliable, efficient, and intelligent interactions, especially in complex RAG workflows and tool-augmented scenarios.**\n",
      "\n",
      "It focuses on:\n",
      "\n",
      "1.  **Structured Context Delivery:** How relevant information (like the chunks from RAG) is presented to the LLM in a clear, unambiguous way.\n",
      "2.  **Role-Based Messaging:** Assigning \"roles\" (user, system, assistant, tool) to different parts of the conversation to guide the LLM's behavior.\n",
      "3.  **Tool and Function Calling:** Allowing the LLM not just to generate text, but to understand when it needs to call an external tool (like a calculator, a search engine, or an API) and how to structure that call.\n",
      "4.  **Managing Dialogue State:** How to keep track of past conversation turns and incorporate them into the current context efficiently.\n",
      "\n",
      "### Why MCP?\n",
      "\n",
      "As LLMs evolve, simple text prompts are often not enough. We need to tell the LLM:\n",
      "*   \"You are an assistant.\" (System role)\n",
      "*   \"Here's the data you need.\" (RAG context)\n",
      "*   \"The user said this.\" (User role)\n",
      "*   \"Last time, you said this.\" (Assistant role)\n",
      "*   \"Oh, and if you need to, you can use these tools to get more info or take actions.\" (Tool definitions)\n",
      "*   \"The result of that tool call was this.\" (Tool output)\n",
      "\n",
      "MCP helps manage this complexity to:\n",
      "\n",
      "*   **Improve Reliability:** By explicitly defining roles and structures, the LLM is less likely to misinterpret instructions.\n",
      "*   **Enable Advanced Features:** Makes function calling and multi-turn conversations much more robust.\n",
      "*   **Enhance Efficiency:** By providing structured context, the LLM can process information more effectively.\n",
      "*   **Promote Interoperability (eventually):** If protocols become more standardized, it's easier to switch between different LLM providers.\n",
      "\n",
      "### How does MCP relate to RAG?\n",
      "\n",
      "RAG is about *finding* the relevant information. MCP is about *packaging and delivering* that information (along with other interaction details) to the LLM in the most effective way.\n",
      "\n",
      "Think of it this way:\n",
      "\n",
      "*   **RAG is the librarian** who goes out, finds the exact books/documents you need, and brings them back to your desk.\n",
      "*   **MCP is the detailed instruction manual** you give the LLM along with those books:\n",
      "    *   \"You are an expert on this topic (System Role).\"\n",
      "    *   \"Here are the relevant sections from the books the librarian found (RAG-provided context).\"\n",
      "    *   \"The user's question is: ... (User Role).\"\n",
      "    *   \"Based ONLY on these sections, answer the user's question. If you need to do a calculation, use the 'calculator' tool like this: `calculator(expression)` (Tool Definition).\"\n",
      "\n",
      "**In essence:** RAG provides the *content* for the context, and MCP (or concepts like it) dictates the *format and structure* of that context for optimal LLM interaction. Many modern LLM APIs (like OpenAI's Chat Completion API) implement aspects of an \"MCP\" by requiring message objects with \"roles\" and allowing \"tool_calls.\"\n",
      "\n",
      "### Key Aspects of a Conceptual MCP (as seen in current LLM APIs):\n",
      "\n",
      "*   **Message Structure:** Instead of a single string prompt, interactions are a list of \"messages,\" each with a `role` (e.g., `system`, `user`, `assistant`, `tool`) and `content`.\n",
      "    *   `system` messages set the overall behavior or provide global context.\n",
      "    *   `user` messages are the direct input from the user.\n",
      "    *   `assistant` messages are previous responses from the LLM, maintaining conversation history.\n",
      "    *   `tool` messages provide the results of tools called by the LLM.\n",
      "*   **Tool/Function Definitions:** You can define a list of functions (tools) the LLM is allowed to \"call.\" The LLM can then choose to call a tool, and your application executes it and passes the result back as a `tool` message.\n",
      "*   **Context Management:** Strategies for deciding which past messages to include in the current prompt (e.g., summarizing older parts of the conversation, using techniques like \"sliding window\" to keep context relevant but within token limits).\n",
      "\n",
      "---\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "**RAG** is your LLM's research assistant, helping it find and incorporate specific, external information to answer questions accurately and avoid hallucinations. It's about bringing the right data *to* the model.\n",
      "\n",
      "**MCP (Model Context Protocol)**, while not a single universal standard, represents the evolved way we structure and manage the *entire conversation* and *all the relevant information* (including the information RAG provides) that we send to and receive from an LLM. It's about how that data is *presented* and how the interaction is *orchestrated*.\n",
      "\n",
      "Both are crucial for building sophisticated, reliable, and intelligent AI applications with LLMs today!\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06375d42",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "- Secure API key management using environment variables\n",
    "- Basic AI API integration\n",
    "- Prompt based text generation\n",
    "- Clean separation of explanation and implementation\n",
    "\n",
    "This serves as a foundational example that can be expanded into chatbots, streaming systems, or larger AI-powered applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
